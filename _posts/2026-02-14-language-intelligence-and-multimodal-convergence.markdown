---
title: "Language, Intelligence, and the Multimodal Convergence"
description: "From dolphins to deep learning, language appears to be a uniquely powerful substrate for developing general reasoning. But it reaches its full potential only when grounded in other forms of experience."
date: 2026-02-14
tags:
  - artificial-intelligence
  - cognitive-science
  - language
  - multimodal-ai
draft: false
---

Something surprising happened on the way to artificial general intelligence. For decades, the AI research community assumed that language understanding would be a *downstream product* of general intelligence—that you'd need to solve reasoning first, and language would follow. Instead, the field discovered that training on language prediction alone produced systems with broad reasoning, coding, math, and planning abilities. Language wasn't the destination. It was the vehicle.

What makes this even more striking is that the biological evidence tells a remarkably similar story.

## Language as a Catalyst for Intelligence

Across species, more complex communication systems correlate with more flexible cognition. [Cetaceans](https://en.wikipedia.org/wiki/Cetacea), great apes, [corvids](https://en.wikipedia.org/wiki/Corvidae), and elephants all rank high on cognitive benchmarks like [mirror self-recognition](https://en.wikipedia.org/wiki/Mirror_test), tool use, and social reasoning—and all have relatively sophisticated signaling systems.

But correlation isn't causation. The stronger evidence comes from intervention studies—experiments where researchers actively train subjects in new skills and measure the resulting cognitive changes. Language-trained apes like Kanzi the bonobo demonstrate cognitive abilities—planning, categorical reasoning, understanding novel concept combinations—that they don't typically display without symbolic training.<sup>[[1](#ref1)]</sup> Language doesn't just reflect intelligence. It actively scaffolds it.<sup>[[6](#ref6)]</sup>

Human development data reinforces this. Children's cognitive growth tracks closely with language acquisition. Inner speech—self-directed language—appears critical for executive function, working memory, and self-regulation. Deaf children without early language exposure show measurable delays in theory of mind, even when general intelligence is unaffected.<sup>[[5](#ref5)]</sup> Language specifically enables certain cognitive capacities that don't emerge on their own.

The evolutionary picture suggests a feedback loop. [Dunbar's social brain hypothesis](https://en.wikipedia.org/wiki/Dunbar%27s_number)<sup>[[2](#ref2)]</sup> and [Tomasello's shared intentionality framework](https://en.wikipedia.org/wiki/Shared_intentionality)<sup>[[3](#ref3)]</sup> both argue that language and intelligence co-evolved. Larger social groups demanded more sophisticated communication, which enabled more complex cooperation and cultural transmission, which rewarded larger social groups. A ratchet effect, accelerating cognitive complexity in social species.

## The AI Mirror

The parallels to artificial intelligence are hard to ignore.

Large language models learn to predict the next token in a sequence of text. That's it. But language is a compressed representation of human knowledge, reasoning patterns, and world models. To predict language well, a model is implicitly forced to learn causal reasoning, spatial relationships, social dynamics, and logical inference. The text is a shadow of the world, and modeling the shadow requires modeling much of the world.

Just as language-trained apes show capabilities they don't display otherwise, language-trained AI systems exhibit emergent abilities—chain-of-thought reasoning, analogical thinking, in-context learning—that weren't present at smaller scales and weren't directly optimized for.<sup>[[10](#ref10)]</sup> The symbolic structure of language scaffolds abstract reasoning in silicon just as it does in brains.

One of the most effective techniques in modern AI is having models "think out loud" step by step before answering.<sup>[[9](#ref9)]</sup> This is strikingly analogous to [Vygotsky's theory of inner speech in human development](https://en.wikipedia.org/wiki/Inner_monologue)—the idea that externalized language gets internalized as a thinking tool.<sup>[[4](#ref4)]</sup> AI models literally reason better when they use language to structure their thinking.

And the limits mirror biological limits too. Current language models struggle with tasks that don't map well to linguistic representation: fine motor planning, continuous spatial reasoning, real-time sensory processing. These are areas where embodied experience matters more than language, just as octopus intelligence suggests non-linguistic routes to cognition exist.<sup>[[13](#ref13)]</sup>

## The Multimodal Convergence

This is where the story gets most interesting. The frontier of AI research is increasingly multimodal—integrating language, vision, audio, and action into unified systems. And the key finding isn't just that these systems *can* handle multiple formats. It's that each modality improves performance in the others. A model that can see images reasons better about spatial language. A model that processes code writes better natural language explanations. The modalities are synergistic, not merely additive.

This addresses one of the deepest critiques of pure language models: the symbol grounding problem.<sup>[[8](#ref8)]</sup> Words in a language-only system are patterns of tokens without real-world reference. Multimodal training partially solves this. A model that has seen millions of images of dogs alongside the word "dog" has something closer to a grounded concept than one that only knows "dog" from its textual relationships to other words.

The biological parallel is exact. Human cognition is fundamentally multimodal. Our concepts aren't stored as text—they're distributed across sensory, motor, and linguistic representations.<sup>[[7](#ref7)]</sup> The brain's convergence zones integrate information across modalities into unified representations. Multimodal AI architectures are converging on the same design principle, not by imitation but by necessity.

The most exciting frontier is robotics. Teams at [Google DeepMind](https://deepmind.google/), [Figure](https://www.figure.ai/), and others are connecting language models to robotic bodies.<sup>[[11](#ref11)]</sup> Language provides a powerful planning and abstraction layer; embodied experience provides the grounding and physics understanding that language alone struggles with. A robot that can be told "pick up the fragile thing carefully" needs language comprehension, visual recognition, and motor control integrated seamlessly.

Some of the most interesting findings involve capabilities that emerge only when modalities are combined—spatial reasoning that pure language models fail at, generalization of instructions to novel visual scenarios never described in text. The whole exceeds the sum of the parts.

## The Throughline

The research trajectory in AI points toward unified world models that maintain a shared internal representation updated by whatever modality is available—much like how your brain seamlessly integrates what you see, hear, feel, and know into a single coherent experience of reality.<sup>[[12](#ref12)]</sup>

The pattern holds across both biological and artificial intelligence: language is the most powerful single modality for abstract reasoning, but it reaches its full potential when grounded in other forms of experience. The fact that this principle holds across such different substrates—carbon and silicon—suggests something deep about the relationship between symbolic communication and intelligence itself.

Language isn't intelligence. But it might be the closest thing to a universal catalyst for it.

---

## References

<a id="ref1"></a>1. Savage-Rumbaugh, S., & Lewin, R. (1994). *Kanzi: The Ape at the Brink of the Human Mind*. Wiley. — Documents Kanzi's language acquisition and the emergent cognitive abilities observed in language-trained bonobos.

<a id="ref2"></a>2. Dunbar, R. I. M. (1998). "The Social Brain Hypothesis." *Evolutionary Anthropology*, 6(5), 178–190. — Proposes that primate brain size evolved primarily to manage complex social relationships, with language as a key enabler.

<a id="ref3"></a>3. Tomasello, M. (2008). *Origins of Human Communication*. MIT Press. — Argues that shared intentionality and cooperative communication are the foundations of uniquely human cognition.

<a id="ref4"></a>4. Vygotsky, L. S. (1934/1986). *Thought and Language*. MIT Press. — The foundational work on inner speech as a cognitive tool, arguing that language transforms thinking rather than merely expressing it.

<a id="ref5"></a>5. Peterson, C. C., & Siegal, M. (2000). "Insights into Theory of Mind from Deafness and Autism." *Mind & Language*, 15(1), 123–145. doi:10.1111/1468-0017.00126 — Demonstrates that deaf children without early language access show delays in theory of mind development.

<a id="ref6"></a>6. Lupyan, G., & Bergen, B. (2016). "How Language Programs the Mind." *Topics in Cognitive Science*, 8(2), 408–424. — Reviews evidence for how language shapes perception, categorization, and memory across domains.

<a id="ref7"></a>7. Barsalou, L. W. (1999). "Perceptual Symbol Systems." *Behavioral and Brain Sciences*, 22(4), 577–660. — Proposes that cognition is grounded in simulated sensory-motor experience rather than amodal symbols.

<a id="ref8"></a>8. Harnad, S. (1990). "The Symbol Grounding Problem." *Physica D*, 42, 335–346. — Defines the problem of how symbols acquire meaning, a central challenge for both AI and cognitive science.

<a id="ref9"></a>9. Wei, J., et al. (2022). "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models." *Advances in Neural Information Processing Systems*, 35, 24824–24837. — Demonstrates that step-by-step verbal reasoning dramatically improves LLM performance on complex tasks.

<a id="ref10"></a>10. Wei, J., et al. (2022). "Emergent Abilities of Large Language Models." *Transactions on Machine Learning Research*. — Documents cognitive capabilities that appear suddenly at scale in language models without being explicitly trained.

<a id="ref11"></a>11. Brohan, A., et al. (2023). "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control." *arXiv preprint arXiv:2307.15818*. — Shows how multimodal language-vision models can be connected to robotic action for grounded intelligence.

<a id="ref12"></a>12. Goyal, A., & Bengio, Y. (2022). "Inductive Biases for Deep Learning of Higher-Level Cognition." *Proceedings of the Royal Society A*, 478(2266). — Discusses architectural principles for building AI systems that develop abstract reasoning, including the role of multimodal integration.

<a id="ref13"></a>13. Mather, J. A., & Dickel, L. (2017). "Cephalopod Complex Cognition." *Current Opinion in Behavioral Sciences*, 16, 131–137. — Reviews evidence for sophisticated intelligence in octopuses and cuttlefish despite minimal social communication.
